import pandas as pd
import nltk
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from nltk.tokenize import word_tokenize
from googleapiclient.discovery import build

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Predefined API key
API_KEY = "AIzaSyBRG5gL7TfzA5MmQmYB0TKwzD__VDdEIWw"

def preprocess_text(text):
    if isinstance(text, str):
        tokens = word_tokenize(text)
        tokens = [word.lower() for word in tokens if word.isalpha()]
        return ' '.join(tokens)
    else:
        return ""  # Return an empty string for non-string values

def fetch_comments(video_id, api_key):
    # Initialize YouTube API client
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    # Fetch comments for the given video ID
    comments = []
    next_page_token = None
    total_comments = 0
    
    while total_comments < 500:
        request = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            maxResults=min(100, 500 - total_comments),  # Limit to 500 comments
            pageToken=next_page_token
        )
        response = request.execute()
        
        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textOriginal']
            comments.append(comment)
            total_comments += 1
            if total_comments >= 500:
                break
        
        if total_comments >= 500 or 'nextPageToken' not in response:
            break
        else:
            next_page_token = response['nextPageToken']
    
    return comments[:500]  # Return at most 500 comments

def main():
    # Load dataset from CSV file
    dataset = pd.read_csv(r'C:\Users\Ayush\Downloads\final_data.csv')
    
    # Preprocess text data
    dataset['Comment'] = dataset['Comment'].apply(preprocess_text)
    
    # Drop rows with empty comments
    dataset = dataset[dataset['Comment'] != ""]
    
    # Split dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(dataset['Comment'], dataset['Sentiment'], test_size=0.2, random_state=42)
    
    # Vectorize text data
    vectorizer = CountVectorizer()
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    
    # Train Naive Bayes classifier
    nb_classifier = MultinomialNB()
    nb_classifier.fit(X_train_vec, y_train)
    
    # Evaluate classifier
    y_pred = nb_classifier.predict(X_test_vec)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy:", accuracy * 100, "%")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # Get YouTube video ID from the user
    youtube_video_id = input("Enter YouTube video ID: ")
    
    # Fetch comments using the predefined API key
    comments_data = fetch_comments(youtube_video_id, API_KEY)
    
    if comments_data:
        # Perform sentiment analysis on YouTube comments
        comments_data_processed = [preprocess_text(comment) for comment in comments_data]
        comments_vec = vectorizer.transform(comments_data_processed)
        sentiments = nb_classifier.predict(comments_vec)
        
        # Print sentences and their sentiments
        print("\nSentences and their Sentiments:")
        for i, comment in enumerate(comments_data):
            print(f"Sentence {i+1}: {comment} - Sentiment: {sentiments[i]}")
        
        # Plot pie chart and bar graph
        sentiment_counts = pd.Series(sentiments).value_counts()
        
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        sentiment_counts.plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])
        plt.title('Sentiment Distribution', fontsize=14)
        
        plt.subplot(1, 2, 2)
        sentiment_counts.plot(kind='bar', color=['skyblue', 'lightcoral'], fontsize=10)
        plt.title('Sentiment Distribution', fontsize=14)
        plt.xlabel('Sentiment', fontsize=12)
        plt.ylabel('Count', fontsize=12)
        
        plt.tight_layout(pad=3.0)
        plt.show()
    else:
        print("No comments found for the provided video ID or less than 500 comments available.")

if __name__ == "__main__":
    main()
